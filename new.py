import numpy as np
import random
import pickle

class MicrofinanceEnv:
    """
    A simulated reinforcement learning environment for dynamic lending policies.
    The environment state consists of a risk threshold and a loan volume.
    Actions:
        0: Decrease risk threshold
        1: No change
        2: Increase risk threshold
    """
    def _init_(self):
        self.state = np.array([0.5, 1000])  
        self.action_space = [0, 1, 2]
        self.penalty_factor = 10
        self.social_factor = 5
        self.base_performance = 100

    def reset(self):
        self.state = np.array([0.5, 1000])
        return self.state

    def step(self, action):
        risk_threshold, loan_volume = self.state
        
        if action == 0:
            risk_threshold = max(0, risk_threshold - 0.05)
        elif action == 2:
            risk_threshold = min(1, risk_threshold + 0.05)
        
        financial_performance = self.base_performance * (1 - risk_threshold)
        risk_penalty = risk_threshold * self.penalty_factor
        social_bonus = (1 - risk_threshold) * self.social_factor
        reward = (financial_performance - risk_penalty) + social_bonus
        
        loan_volume = loan_volume * (1 + (risk_threshold - 0.5) * 0.1)
        
        self.state = np.array([risk_threshold, loan_volume])
        done = False  
        return self.state, reward, done, {}

def train_rl_model(num_episodes=1000, learning_rate=0.1, discount_factor=0.95, epsilon=0.1, output_path='rl_agent.pkl'):
    """
    Trains a Q-learning agent in the MicrofinanceEnv.
    Synthetic data is generated by simulating episodes using the environment's knowledge base.
    """
    env = MicrofinanceEnv()
    
    risk_bins = np.linspace(0, 1, 11)
    state_space_size = len(risk_bins)
    action_space_size = len(env.action_space)
    Q_table = np.zeros((state_space_size, action_space_size))

    def discretize_state(state):
        risk_threshold, _ = state
        index = np.digitize(risk_threshold, risk_bins) - 1
        return max(0, min(index, state_space_size - 1))
    
    for episode in range(num_episodes):
        state = env.reset()
        state_idx = discretize_state(state)
        total_reward = 0
        
        for step in range(50):
            if random.random() < epsilon:
                action = random.choice(env.action_space)
            else:
                action = np.argmax(Q_table[state_idx])
            
            next_state, reward, done, _ = env.step(action)
            next_state_idx = discretize_state(next_state)
            
            Q_table[state_idx, action] = Q_table[state_idx, action] + learning_rate * (
                reward + discount_factor * np.max(Q_table[next_state_idx]) - Q_table[state_idx, action]
            )
            
            state_idx = next_state_idx
            total_reward += reward
        
        if episode % 100 == 0:
            print(f"Episode {episode}: Total Reward: {total_reward}")
    
    with open(output_path, 'wb') as f:
        pickle.dump(Q_table, f)
    print(f"RL agent trained and saved to {output_path}.")
    return Q_table

if __name__ == "_main_":
    train_rl_model(num_episodes=500) 